{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Zero: Env Setup & API Keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conda Setup\n",
    "```\n",
    "conda create -n chatbot python=3.11 # 创建虚拟环境，其中chatbot是虚拟环境的名字，可自定义；python版本习惯选择3.8-3.11版本，兼容性最好。\n",
    "conda activate chatbot # 激活虚拟环境\n",
    "```\n",
    "### Poetry Setup\n",
    "\n",
    "pip install poetry\n",
    "poetry commands & setup:\n",
    "\n",
    "```\n",
    "poetry new <project_name> # 新建项目\n",
    "poetry init # 初始化项目\n",
    "poetry install --no-root # 安装依赖\n",
    "poetry add <package_name> # 添加依赖\n",
    "poetry remove <package_name> # 移除依赖\n",
    "poetry show # 查看依赖\n",
    "```\n",
    "\n",
    "已有Poetry文件，安装依赖：`poetry install --no-root`\n",
    "\n",
    "### .env \n",
    "参考.env.example文件，创建自己的.env文件\n",
    "```\n",
    "DS_API_KEY= <your key>\n",
    "GROQ_API_KEY= <your key>\n",
    "```\n",
    "### Ollama Setup\n",
    "https://ollama.com/download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One: Connecting 2 LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lc V0.3: The main change in 0.3 is migrating to Pydantic 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()\n",
    "#os.getenv(\"API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to deepseek model\n",
    "llm = init_chat_model(\n",
    "    model=\"deepseek-chat\",\n",
    "    model_provider=\"openai\",\n",
    "    api_key=os.environ.get('DS_API_KEY'),\n",
    "    base_url=\"https://api.deepseek.com\",\n",
    "    temperature = 0.2,\n",
    "    top_p=0.9,\n",
    "    max_tokens=1024,\n",
    "    stop=[\"<|endoftext|>\", \"<|endofcode|>\"],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# invoke the model\n",
    "response = llm.invoke(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke the model\n",
    "response = llm.invoke(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 9, 'total_tokens': 45, 'completion_tokens_details': None, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 9}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_1c141eb703', 'finish_reason': 'stop', 'logprobs': None}, id='run-160f95e1-f478-4c4d-91f7-78722eb50a41-0', usage_metadata={'input_tokens': 9, 'output_tokens': 36, 'total_tokens': 45})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I\\'m just a language model, I don\\'t have feelings or emotions like humans do, so I don\\'t have good or bad days. I\\'m always \"on\" and ready to help with any questions or tasks you may have! How can I assist you today?'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = init_chat_model(\n",
    "    temperature=0.2,\n",
    "    model=\"llama3-70b-8192\",\n",
    "    model_provider=\"groq\",\n",
    "    api_key = os.environ.get('GROQ_API_KEY'),\n",
    ")\n",
    "\n",
    "llm.invoke(\"Hello, how are you?\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sungu\\AppData\\Local\\Temp\\ipykernel_27556\\2915866486.py:1: LangChainBetaWarning: The function `init_chat_model` is in beta. It is actively being worked on, so the API may change.\n",
      "  llm = init_chat_model(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "llm = init_chat_model(\n",
    "    model=\"llama3\",\n",
    "    model_provider=\"ollama\",\n",
    "    # api_key  os.envir=on.get('GROQ_API_KEY'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm just an AI, so I don't have feelings like humans do, but I'm functioning properly and ready to help with any questions or tasks you may have! It's great to chat with you. How about you - what brings you here today?\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Hello, how are you?\").content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Two: Real-time Chatbot with Chat History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sungu\\AppData\\Local\\Temp\\ipykernel_32672\\2196674182.py:5: LangChainBetaWarning: The function `init_chat_model` is in beta. It is actively being worked on, so the API may change.\n",
      "  llm = init_chat_model(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "llm = init_chat_model(\n",
    "    model=\"deepseek-chat\",\n",
    "    model_provider=\"openai\",\n",
    "    api_key=os.environ.get('DS_API_KEY'),\n",
    "    base_url=\"https://api.deepseek.com\",\n",
    "    temperature = 0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 4, 'total_tokens': 13, 'completion_tokens_details': None, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 4}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_1c141eb703', 'finish_reason': 'stop', 'logprobs': None}, id='run-f5a918f6-6cd8-42ad-9173-caf687cdd195-0', usage_metadata={'input_tokens': 4, 'output_tokens': 9, 'total_tokens': 13})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke('Hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage \n",
    "# same as 'from langchain.schema import SystemMessage, HumanMessage, AIMessage'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup system prompt\n",
    "system_message = SystemMessage(content='You are a helpful assistant. Try to think step by step whenever possible')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "chat_history.append(system_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, I am Lloyd\n",
      "Hello Lloyd! How can I assist you today?\n",
      "What is my name\n",
      "Your name is Lloyd. How can I help you with anything today?\n",
      "which one is bigger, 9.8 or 9.11\n",
      "9.8 is bigger than 9.11. Here's a step-by-step comparison:\n",
      "\n",
      "1. Both numbers have the same number of digits before the decimal point (9).\n",
      "2. Compare the digits after the decimal point:\n",
      "   - For 9.8, the digit after the decimal point is 8.\n",
      "   - For 9.11, the digits after the decimal point are 1 and 1.\n",
      "3. Since 8 is greater than 1, 9.8 is larger than 9.11.\n",
      "---- Message History ----\n",
      "[SystemMessage(content='You are a helpful assistant. Try to think step by step whenever possible', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hi, I am Lloyd', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello Lloyd! How can I assist you today?', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is my name', additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is Lloyd. How can I help you with anything today?', additional_kwargs={}, response_metadata={}), HumanMessage(content='which one is bigger, 9.8 or 9.11', additional_kwargs={}, response_metadata={}), AIMessage(content=\"9.8 is bigger than 9.11. Here's a step-by-step comparison:\\n\\n1. Both numbers have the same number of digits before the decimal point (9).\\n2. Compare the digits after the decimal point:\\n   - For 9.8, the digit after the decimal point is 8.\\n   - For 9.11, the digits after the decimal point are 1 and 1.\\n3. Since 8 is greater than 1, 9.8 is larger than 9.11.\", additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "chat_history.append(system_message)\n",
    "while True:\n",
    "    query = input('User:')\n",
    "    if query.lower() == \"exit\":\n",
    "        break\n",
    "    print(query)\n",
    "\n",
    "    chat_history.append(HumanMessage(content=query))\n",
    "    response = llm.invoke(chat_history)\n",
    "    \n",
    "    print(response.content)\n",
    "    chat_history.append(AIMessage(content=response.content))\n",
    "\n",
    "print(\"---- Message History ----\")\n",
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "183ef8501b73446b824e658aac74406c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=''), Text(value='', placeholder='Type your message here...'), Button(description='Se…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import widgets\n",
    "from IPython.display import display, clear_output\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "# Setup system prompt\n",
    "system_message = SystemMessage(content='You are a helpful assistant. Try to think step by step whenever possible')\n",
    "\n",
    "# Create input box and send button\n",
    "input_box = widgets.Text(placeholder='Type your message here...')\n",
    "send_button = widgets.Button(description='Send')\n",
    "output = widgets.Output()\n",
    "\n",
    "# Create chat history display area\n",
    "chat_history_display = widgets.HTML()\n",
    "\n",
    "# Store conversation history\n",
    "chat_history = [system_message]\n",
    "\n",
    "# Define send message function\n",
    "def send_message(b):\n",
    "    query = input_box.value\n",
    "    input_box.value = ''  # Clear input box\n",
    "    \n",
    "    if query.lower() == \"exit\":\n",
    "        return\n",
    "    \n",
    "    chat_history.append(HumanMessage(content=query))\n",
    "    \n",
    "    response = llm.invoke(chat_history)\n",
    "    chat_history.append(AIMessage(content=response.content))\n",
    "    \n",
    "    # Update chat history display\n",
    "    chat_history_display.value += f\"<p><strong>User:</strong> {query}</p>\"\n",
    "    chat_history_display.value += f\"<p><strong>AI:</strong> {response.content}</p>\"\n",
    "\n",
    "# Bind send button event\n",
    "send_button.on_click(send_message)\n",
    "\n",
    "# Create chat interface layout\n",
    "chat_interface = widgets.VBox([chat_history_display, input_box, send_button])\n",
    "\n",
    "# Display chat interface\n",
    "display(chat_interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant. Try to think step by step whenever possible', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hi, I am lloyd', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Hello Lloyd! How can I assist you today?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Introduce yourself ', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Hello Lloyd! My name is ChatGPT, and I'm an AI assistant here to help you with a wide range of tasks, from answering questions and providing information to assisting with creative writing and problem-solving. How can I assist you today?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Introduce yourself. As you are intending a job interview', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Certainly! Here's a brief introduction for a job interview:\\n\\n---\\n\\nGood [morning/afternoon],\\n\\nMy name is Lloyd, and I am excited about the opportunity to interview for this position. With [X years] of experience in [your field or industry], I have developed a strong foundation in [key skills or areas relevant to the job]. My background includes roles where I successfully [mention a specific achievement or responsibility], which I believe aligns well with the requirements of this role.\\n\\nI am particularly drawn to this position because of [mention a specific aspect of the company or role that interests you]. I am passionate about [mention a relevant interest or area of expertise], and I am confident that my skills and experiences make me a strong candidate for this opportunity.\\n\\nThank you for considering my application. I look forward to the possibility of contributing to your team and helping to achieve [company's goals or mission].\\n\\n---\\n\\nFeel free to customize this introduction to better fit your personal experiences and the specific job you are applying for. Good luck with your interview!\", additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Three: Managing Chat History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Message History ----\n",
      "[SystemMessage(content='You are a helpful assistant. Try to think step by step whenever possible', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "def manage_chat_history(chat_history, chat_length):\n",
    "    # Ensure system message is always present\n",
    "    system_message = chat_history[0]\n",
    "    \n",
    "    # Calculate the number of complete conversation rounds\n",
    "    conversation_rounds = min(chat_length, (len(chat_history) - 1) // 2)\n",
    "    \n",
    "    # Keep only the specified number of conversation rounds\n",
    "    managed_history = [system_message] + chat_history[-(conversation_rounds * 2):]\n",
    "    \n",
    "    return managed_history\n",
    "\n",
    "chat_length = 3  # Set the desired number of conversation rounds to keep\n",
    "chat_history = [system_message]\n",
    "\n",
    "while True:\n",
    "    query = input('User: ')\n",
    "    if query.lower() == \"exit\":\n",
    "        break\n",
    "    print(query)\n",
    "\n",
    "    chat_history.append(HumanMessage(content=query))\n",
    "    chat_history = manage_chat_history(chat_history, chat_length)\n",
    "    \n",
    "    response = llm.invoke(chat_history)\n",
    "    \n",
    "    print(\"AI:\", response.content)\n",
    "    chat_history.append(AIMessage(content=response.content))\n",
    "    chat_history = manage_chat_history(chat_history, chat_length)\n",
    "\n",
    "print(\"---- Message History ----\")\n",
    "print(chat_history)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
